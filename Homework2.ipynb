{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6850dd6f",
   "metadata": {},
   "source": [
    "# Homework 2 Solution \n",
    "1.Show that the stationary point (zero gradient) of the function$$\n",
    "\\begin{aligned}\n",
    "    f=2x_{1}^{2} - 4x_1 x_2+ 1.5x^{2}_{2}+ x_2\n",
    "\\end{aligned}\n",
    "$$is a saddle (with indefinite Hessian). Find the directions of downslopes away from the saddle. Hint: Use Taylor's expansion at the saddle point. Find directions that reduce $f$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "506ba43d",
   "metadata": {},
   "source": [
    "![](p1pg1.jpeg)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae044aba",
   "metadata": {},
   "source": [
    "![](p1pg2.jpeg)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6231e40",
   "metadata": {},
   "source": [
    "![](p1pg3.jpeg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbe60cf6",
   "metadata": {},
   "source": [
    "2. Find the point in the plane $x_1+2x_2+3x_3=1$ in $\\mathbb{R}^3$ that is nearest to the point $(-1,0,1)^T$. Is this a convex problem? \n",
    "\n",
    " Implement the gradient descent and Newton's algorithm for solving the problem. Attach your codes along with a short summary including (1) the initial points tested, (2) corresponding solutions, (3) a log-linear convergence plot.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88095a93",
   "metadata": {},
   "source": [
    "![](p2pg1.jpeg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ded45a69",
   "metadata": {},
   "source": [
    "![](p2pg2.jpeg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "361f37b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "Gradient descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "7f561d69",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-23-7d8867af2ed5>, line 24)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-23-7d8867af2ed5>\"\u001b[1;36m, line \u001b[1;32m24\u001b[0m\n\u001b[1;33m    while phi(a,x)<obj(x-a*np.dot(np.transpose(grad(x))):\u001b[0m\n\u001b[1;37m    ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "#2b\n",
    "# Problem 2\n",
    "import numpy as np \n",
    "\n",
    "obj = lambda x: 5*x[0]**2 + 10*x[1]**2 + (12*x[0]*x[1]) - 8*x[0] - 14*x[1] + 5\n",
    "\n",
    "def grad(x): \n",
    "    \n",
    "    return(10*x[0]+12*x[1]-8),(12*x[0]+20*x[1]-14)  \n",
    "eps = 1e-3  # termination criterion\n",
    "x0 = [0,0]  # initial guess\n",
    "k = 0  # counter\n",
    "soln = [x0]  # use an array to store the search steps\n",
    "x = soln[k]  # start with the initial guess\n",
    "error = np.linalg.norm(grad(x))  \n",
    "a=1\n",
    "\n",
    "# Armijo line search\n",
    "def line_search(x):\n",
    "    a = 1 \n",
    "    h=([[10,12],[12,20]])\n",
    "    d=np.matmul(np.linalg.inv(h),grad(x))\n",
    "    phi = lambda a, x: obj(x) - a*0.8*np.dot(np.transpose(grad(x),d)  # define phi as a search criterion\n",
    "    while phi(a,x)<obj(x-a*np.dot(np.transpose(grad(x))):  \n",
    "        a = 0.5*a\n",
    "    return a\n",
    "\n",
    "while error >= eps:  # keep searching while gradient norm is larger than eps\n",
    "    a = line_search(x)\n",
    "    x = x -a*grad(x)\n",
    "    soln.append(x)\n",
    "    error = no.linalg.norm(grad(x))\n",
    "\n",
    "soln     \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dde21ad4",
   "metadata": {},
   "source": [
    "Newton algorithm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd0de689",
   "metadata": {},
   "outputs": [],
   "source": [
    "#2b\n",
    "# Problem 2\n",
    "import numpy as np \n",
    "\n",
    "obj = lambda x: 5*x[0]**2 + 10*x[1]**2 + (12*x[0]*x[1]) - 8*x[0] - 14*x[1] + 5\n",
    "\n",
    "def grad(x): \n",
    "    \n",
    "    return(10*x[0]+12*x[1]-8),(12*x[0]+20*x[1]-14)  \n",
    "eps = 1e-3  # termination criterion\n",
    "x0 = [0,0]  # initial guess\n",
    "k = 0  # counter\n",
    "soln = [x0]  # use an array to store the search steps\n",
    "x = soln[k]  # start with the initial guess\n",
    "error = np.linalg.norm(grad(x))  \n",
    "a=1\n",
    "\n",
    "# Armijo line search\n",
    "def line_search(x):\n",
    "    a = 1 \n",
    "    h=([[10,12],[12,20]])\n",
    "    phi = lambda a, x: obj(x) - a*0.8*grad(x)**2  # define phi as a search criterion\n",
    "    while phi(a,x)<obj(x-a*np.dot(np.transpose(grad(x))):  \n",
    "        a = 0.5*a\n",
    "    return a\n",
    "\n",
    "while error >= eps:  # keep searching while gradient norm is larger than eps\n",
    "    a = line_search(x)\n",
    "    x = x -a*grad(x)\n",
    "    soln.append(x)\n",
    "    error = no.linalg.norm(grad(x))\n",
    "\n",
    "soln     \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23b810d2",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d64ddf60",
   "metadata": {},
   "source": [
    "3.Let $f(x)$ and $g(x)$ be two convex functions defined on the convex set $\\mathcal{X}$.\n",
    "\n",
    "  Prove that $af(x)+bg(x)$ is convex for $a>0$ and $b>0$.\n",
    "  \n",
    "  In what conditions will $f(g(x))$ be convex?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48d28ae4",
   "metadata": {},
   "source": [
    "![](p3pg1.jpeg)\n",
    "![](p3pg2.jpeg)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97ddb75e",
   "metadata": {},
   "source": [
    "4.Show that $f({\\bf x}_1) \\geq f(\\textbf{x}_0) + \n",
    "    \\textbf{g}_{\\textbf{x}_0}^T(\\textbf{x}_1-\\textbf{x}_0)$ for a convex function $f(\\textbf{x}): \\mathcal{X} \\rightarrow \\mathbb{R}$ and for $\\textbf{x}_0$, $\\textbf{x}_1 \\in \\mathcal{X}$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0ac15ed",
   "metadata": {},
   "source": [
    "![](p4pg0.jpeg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a394f39",
   "metadata": {},
   "source": [
    "![](p4pg1.jpeg)\n",
    "![](p4pg2.jpeg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d08dd53f",
   "metadata": {},
   "source": [
    "5.Consider an illumination problem: There are $n$ lamps and $m$ mirrors fixed to the ground. The target reflection intensity level is $I_t$. The actual reflection intensity level on the $k$th mirror can be computed as $\\textbf{a}_k^T \\textbf{p}$, where $\\textbf{a}_k$ is given by the distances between all lamps to the mirror, and $\\textbf{p}:=[p_1,...,p_n]^T$ are the power output of the lamps. The objective is to keep the actual intensity levels as close to the target as possible by tuning the power output $\\textbf{p}$.\n",
    "\n",
    "(5 points) Formulate this problem as an optimization problem.\n",
    "(5 points) Is your problem convex?\n",
    "(5 points) If we require the overall power output of any of the $n$ lamps to be less than $p^*$, will the problem have a unique solution?\n",
    "(5 points) If we require no more than half of the lamps to be switched on, will the problem have a unique solution?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3db93f2",
   "metadata": {},
   "source": [
    "![](p5pg1.jpeg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ac4369d",
   "metadata": {},
   "source": [
    "c) if the total power of n number lamps is less than p*, we can state that it has unique solution.\n",
    "d)if we have to turn on more lamps, then it wont be a convex problem."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
